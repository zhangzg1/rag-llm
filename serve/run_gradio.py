import sys

sys.path.append("../")
import gradio as gr
from qa_chain.model_to_llm import model_to_llm
from qa_chain.get_vectordb import get_vectordb
from qa_chain.Chat_QA_chain_self import Chat_QA_chain_self
from qa_chain.QA_chain_self import QA_chain_self
from typing import Any
import re
import os
from dotenv import load_dotenv

load_dotenv()

api_key = os.getenv("zhipu_api_key")

LLM_MODEL_DICT = {
    # "openai": ["gpt-3.5-turbo", "gpt-3.5-turbo-16k-0613", "gpt-3.5-turbo-0613", "gpt-4", "gpt-4-32k"],
    # "wenxin": ["ERNIE-Bot", "ERNIE-Bot-4", "ERNIE-Bot-turbo"],
    # "xinhuo": ["Spark-1.5", "Spark-2.0"],
    # "llama": ["Atom-7b", "Llama3-8b"],
    # "zhipuai": ["chatglm_pro", "chatglm_std", "chatglm_lite"]
    "openai": ["gpt-3.5-turbo", "gpt-4"],
    "wenxin": ["ERNIE-Bot"],
    "xinhuo": ["Spark-2.0"],
    "llama": ["Atom-7b", "Llama3-8b"],
    "zhipuai": ["chatglm_pro", "chatglm_std"]
}

LLM_MODEL_LIST = sum(list(LLM_MODEL_DICT.values()), [])
INIT_LLM = "chatglm_std"
EMBEDDING_MODEL_LIST = ['zhipuai', 'openai', 'm3e']
INIT_EMBEDDING_MODEL = "zhipuai"
DEFAULT_DB_PATH = "/home/zhangzg/LLM/rags/database/data/Introduction.md"
DEFAULT_PERSIST_PATH = "/home/zhangzg/LLM/rags/vector_db/ok"
AIGC_AVATAR_PATH = "/home/zhangzg/LLM/rags/figures/datawhale_avatar.png"
DATAWHALE_AVATAR_PATH = "/home/zhangzg/LLM/rags/figures/datawhale_avatar.png"
AIGC_LOGO_PATH = "/home/zhangzg/LLM/rags/figures/aigc_logo.png"
DATAWHALE_LOGO_PATH = "/home/zhangzg/LLM/rags/figures/datawhale_logo.png"


def get_model_by_platform(platform):
    return LLM_MODEL_DICT.get(platform, "")


class Model_center():
    """
    å­˜å‚¨é—®ç­” Chain çš„å¯¹è±¡
    - chat_qa_chain_self: ä»¥ (model, embedding_model) ä¸ºé”®å­˜å‚¨çš„å¸¦å†å²è®°å½•çš„é—®ç­”é“¾ã€‚
    - qa_chain_self: ä»¥ (model, embedding_model) ä¸ºé”®å­˜å‚¨çš„ä¸å¸¦å†å²è®°å½•çš„é—®ç­”é“¾ã€‚
    """

    def __init__(self):
        self.chat_qa_chain_self = {}
        self.qa_chain_self = {}

    def chat_qa_chain_self_answer(self, question: str, chat_history: list = [], model: str = "chatglm_std",
                                  embedding_model: str = "zhipuai", temperature: float = 0.0, top_k: int = 2,
                                  file_path: str = DEFAULT_DB_PATH, persist_path: str = DEFAULT_PERSIST_PATH,
                                  vectordb: Any = None, api_key=api_key):
        """
        è°ƒç”¨å¸¦å†å²è®°å½•çš„é—®ç­”é“¾è¿›è¡Œå›ç­”
        """
        if question == None or len(question) < 1:
            return "", chat_history
        try:
            if (model, embedding_model) not in self.chat_qa_chain_self:
                self.chat_qa_chain_self[(model, embedding_model)] = Chat_QA_chain_self(model=model, top_k=top_k,
                                                                                       temperature=temperature,
                                                                                       chat_history=chat_history,
                                                                                       embedding_model=embedding_model,
                                                                                       file_path=file_path,
                                                                                       persist_path=persist_path,
                                                                                       api_key=api_key,
                                                                                       vectordb=vectordb)
            chain = self.chat_qa_chain_self[(model, embedding_model)]
            return "", chain.answer(question=question)
        except Exception as e:
            return e, chat_history

    def qa_chain_self_answer(self, question: str, chat_history: list = [], model: str = "chatglm_std",
                             embedding_model: str = "zhipuai", temperature: float = 0.0, top_k: int = 2,
                             file_path: str = DEFAULT_DB_PATH, persist_path: str = DEFAULT_PERSIST_PATH,
                             vectordb: Any = None, api_key=api_key):
        """
        è°ƒç”¨ä¸å¸¦å†å²è®°å½•çš„é—®ç­”é“¾è¿›è¡Œå›ç­”
        """
        if question == None or len(question) < 1:
            return "", chat_history
        try:
            if (model, embedding_model) not in self.qa_chain_self:
                self.qa_chain_self[(model, embedding_model)] = QA_chain_self(model=model, temperature=temperature,
                                                                             top_k=top_k, api_key=api_key,
                                                                             embedding_model=embedding_model,
                                                                             file_path=file_path,
                                                                             persist_path=persist_path,
                                                                             vectordb=vectordb)
            chain = self.qa_chain_self[(model, embedding_model)]
            chat_history.append(
                (question, chain.answer(question)))
            return "", chat_history
        except Exception as e:
            return e, chat_history

    def clear_history(self):
        if len(self.chat_qa_chain_self) > 0:
            for chain in self.chat_qa_chain_self.values():
                chain.clear_history()


def get_vectordb_info(file_path=DEFAULT_DB_PATH, embedding_model: str = None, persist_path=DEFAULT_PERSIST_PATH):
    if embedding_model == "zhipuai" or embedding_model == "m3e":
        vectordb = get_vectordb(file_path, persist_path, embedding_model)
    return ""


def format_chat_prompt(message, chat_history):
    """
    è¯¥å‡½æ•°ç”¨äºæ ¼å¼åŒ–èŠå¤© promptã€‚

    å‚æ•°:
    message: å½“å‰çš„ç”¨æˆ·æ¶ˆæ¯ã€‚
    chat_history: èŠå¤©å†å²è®°å½•ã€‚

    è¿”å›:
    prompt: æ ¼å¼åŒ–åçš„ promptã€‚
    """
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºå­—ç¬¦ä¸²ï¼Œç”¨äºå­˜æ”¾æ ¼å¼åŒ–åçš„èŠå¤© promptã€‚
    prompt = ""
    # éå†èŠå¤©å†å²è®°å½•ã€‚
    for turn in chat_history:
        # ä»èŠå¤©è®°å½•ä¸­æå–ç”¨æˆ·å’Œæœºå™¨äººçš„æ¶ˆæ¯ã€‚
        user_message, bot_message = turn
        # æ›´æ–° promptï¼ŒåŠ å…¥ç”¨æˆ·å’Œæœºå™¨äººçš„æ¶ˆæ¯ã€‚
        prompt = f"{prompt}\nUser: {user_message}\nAssistant: {bot_message}"
    # å°†å½“å‰çš„ç”¨æˆ·æ¶ˆæ¯ä¹ŸåŠ å…¥åˆ° promptä¸­ï¼Œå¹¶é¢„ç•™ä¸€ä¸ªä½ç½®ç»™æœºå™¨äººçš„å›å¤ã€‚
    prompt = f"{prompt}\nUser: {message}\nAssistant:"
    # è¿”å›æ ¼å¼åŒ–åçš„ promptã€‚
    return prompt


def respond(message, chat_history, model: str = "chatglm_std", history_len=3, temperature=0.1, api_key=api_key):
    """
    è¯¥å‡½æ•°ç”¨äºç”Ÿæˆæœºå™¨äººçš„å›å¤ã€‚

    å‚æ•°:
    message: å½“å‰çš„ç”¨æˆ·æ¶ˆæ¯ã€‚
    chat_history: èŠå¤©å†å²è®°å½•ã€‚

    è¿”å›:
    "": ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºæ²¡æœ‰å†…å®¹éœ€è¦æ˜¾ç¤ºåœ¨ç•Œé¢ä¸Šï¼Œå¯ä»¥æ›¿æ¢ä¸ºçœŸæ­£çš„æœºå™¨äººå›å¤ã€‚
    chat_history: æ›´æ–°åçš„èŠå¤©å†å²è®°å½•
    """
    if message == None or len(message) < 1:
        return "", chat_history
    try:
        # é™åˆ¶ history çš„è®°å¿†é•¿åº¦
        chat_history = chat_history[-history_len:] if history_len > 0 else []
        # è°ƒç”¨ä¸Šé¢çš„å‡½æ•°ï¼Œå°†ç”¨æˆ·çš„æ¶ˆæ¯å’ŒèŠå¤©å†å²è®°å½•æ ¼å¼åŒ–ä¸ºä¸€ä¸ª promptã€‚
        formatted_prompt = format_chat_prompt(message, chat_history)
        # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›å¤
        llm = model_to_llm(model=model, temperature=temperature, api_key=api_key)
        bot_message = llm(formatted_prompt)
        # å°†bot_messageä¸­\næ¢ä¸º<br/>
        bot_message = re.sub(r"\\n", '<br/>', bot_message)
        # å°†ç”¨æˆ·çš„æ¶ˆæ¯å’Œæœºå™¨äººçš„å›å¤åŠ å…¥åˆ°èŠå¤©å†å²è®°å½•ä¸­ã€‚
        chat_history.append((message, bot_message))
        # è¿”å›ä¸€ä¸ªç©ºå­—ç¬¦ä¸²å’Œæ›´æ–°åçš„èŠå¤©å†å²è®°å½•ï¼ˆè¿™é‡Œçš„ç©ºå­—ç¬¦ä¸²å¯ä»¥æ›¿æ¢ä¸ºçœŸæ­£çš„æœºå™¨äººå›å¤ï¼Œå¦‚æœéœ€è¦æ˜¾ç¤ºåœ¨ç•Œé¢ä¸Šï¼‰ã€‚
        return "", chat_history
    except Exception as e:
        return e, chat_history


model_center = Model_center()

block = gr.Blocks()
with block as demo:
    with gr.Row(equal_height=True):
        gr.Image(value=AIGC_LOGO_PATH, scale=1, min_width=10, show_label=False, show_download_button=False,
                 container=False)

        with gr.Column(scale=2):
            gr.Markdown("""<h1><center>                                                   RAGåº”ç”¨ç³»ç»ŸğŸ¦œğŸ”—</center></h1>
                <center>LLMs-RAG</center>
                """)

        gr.Image(value=DATAWHALE_LOGO_PATH, scale=1, min_width=10, show_label=False, show_download_button=False,
                 container=False)

    with gr.Row():
        with gr.Column(scale=4):
            chatbot = gr.Chatbot(height=400, show_copy_button=True, show_share_button=True,
                                 avatar_images=(AIGC_AVATAR_PATH, DATAWHALE_AVATAR_PATH))

            # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
            msg = gr.Textbox(label="Prompt/é—®é¢˜")

            with gr.Row():
                # åˆ›å»ºæäº¤æŒ‰é’®ã€‚
                db_with_his_btn = gr.Button("Chat db with history")
                db_wo_his_btn = gr.Button("Chat db without history")
                llm_btn = gr.Button("Chat with llm")
            with gr.Row():
                # åˆ›å»ºä¸€ä¸ªæ¸…é™¤æŒ‰é’®ï¼Œç”¨äºæ¸…é™¤èŠå¤©æœºå™¨äººç»„ä»¶çš„å†…å®¹ã€‚
                clear = gr.ClearButton(
                    components=[chatbot], value="Clear console")

        with gr.Column(scale=1):
            file = gr.File(label='è¯·é€‰æ‹©çŸ¥è¯†åº“ç›®å½•', file_count='single',
                           file_types=['.txt', '.md', '.docx', '.pdf'])

            with gr.Row():
                init_db = gr.Button("çŸ¥è¯†åº“æ–‡ä»¶å‘é‡åŒ–")
            model_argument = gr.Accordion("å‚æ•°é…ç½®", open=False)
            with model_argument:
                temperature = gr.Slider(0,
                                        1,
                                        value=0.01,
                                        step=0.01,
                                        label="llm temperature",
                                        interactive=True)

                top_k = gr.Slider(1,
                                  10,
                                  value=3,
                                  step=1,
                                  label="vector db search top k",
                                  interactive=True)

                history_len = gr.Slider(0,
                                        5,
                                        value=3,
                                        step=1,
                                        label="history length",
                                        interactive=True)

            model_select = gr.Accordion("æ¨¡å‹é€‰æ‹©")
            with model_select:
                model = gr.Dropdown(
                    LLM_MODEL_LIST,
                    label="large language model",
                    value=INIT_LLM,
                    interactive=True)

                embedding_model = gr.Dropdown(EMBEDDING_MODEL_LIST,
                                              label="Embedding model",
                                              value=INIT_EMBEDDING_MODEL)

        """
            1ã€å½“ç‚¹å‡»â€œçŸ¥è¯†åº“å‘é‡åŒ–â€æ—¶ï¼Œä¼šå°†ä½ é€‰ä¸­çš„æ–‡ä»¶çš„å†…å®¹è¿›è¡Œå‘é‡åŒ–ï¼Œç„¶åå­˜å‚¨åˆ°æ•°æ®åº“ä¸­ã€‚ä¹Ÿå°±æ˜¯å­˜å‚¨åœ¨ DEFAULT_PERSIST_PATH è¿™ä¸ªè·¯å¾„ä¸‹ã€‚
            2ã€å½“ä½ è¿›è¡Œæ£€ç´¢é—®ç­”æ—¶ï¼Œä¼šå°†é»˜è®¤çš„æ–‡ä»¶ï¼ˆ DEFAULT_DB_PATH è·¯å¾„ï¼‰è¿›è¡Œå‘é‡åŒ–ï¼Œç„¶åä¹Ÿå­˜å‚¨åœ¨ DEFAULT_PERSIST_PATH è¿™ä¸ªè·¯å¾„ä¸‹ï¼Œ
            æ­¤æ—¶è¿™ä¸ªè·¯å¾„ä¸‹å°±æœ‰äº†ä¸¤ä¸ªæ–‡ä»¶çš„å‘é‡åŒ–æ•°æ®äº†ã€‚æ‰€ä»¥æ¯å½“ä½ æ”¾å…¥æ–‡ä»¶ç‚¹å‡»â€œå‘é‡åŒ–â€æ—¶å°±ä¼šå°†æ–°æ–‡ä»¶çš„å†…å®¹å‘é‡åŒ–å¹¶ä¸”å­˜å‚¨åœ¨è·¯å¾„ä¸‹ï¼Œç„¶åæ¯æ¬¡ç‚¹å‡»
            æ£€ç´¢é—®ç­”æ—¶ï¼Œå°±ä¼šå°†é»˜è®¤æ–‡ä»¶å‘é‡åŒ–ç„¶åå­˜å‚¨åœ¨è¿™ä¸ªè·¯å¾„ä¸‹ã€‚
            3ã€å•ç‹¬å’Œå¤§æ¨¡å‹èŠå¤©æ—¶ï¼Œä¸ä¼šä½¿ç”¨æ•°æ®åº“ä¸­çš„å‘é‡åŒ–æ•°æ®ï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œå›ç­”ã€‚ä½†æ˜¯å’Œå¤§æ¨¡å‹èŠå¤©æ˜¯ä¼šç»“åˆä½ çš„å†å²è®°å½•è¿›è¡Œå›ç­”çš„ã€‚
            æ‰€ä»¥å¯ä»¥å…ˆæ¸…é™¤å†å²è®°å½•ï¼Œç„¶åå†å’Œå¤§æ¨¡å‹èŠå¤©ã€‚
            4ã€è¿›è¡Œå¸¦å†å²çš„æ£€ç´¢èŠå¤©æ—¶ï¼Œè¦å…ˆæ¸…ç©ºå†å²è®°å½•ï¼Œç„¶åå†è¿›è¡Œæ£€ç´¢èŠå¤©ã€‚ä¸ç„¶æ— æ³•æ­£å¸¸ä½¿ç”¨ã€‚
            5ã€å¸¦å†å²æ£€ç´¢èŠå¤©å’Œç›´æ¥ä¸å¤§æ¨¡å‹èŠå¤©è¿™ä¸¤ä¸ªéƒ½æ˜¯ä¼šç»“åˆèŠå¤©è®°å½•çš„ï¼Œå¸¦å†å²æ£€ç´¢å’Œä¸å¸¦å†å²æ£€ç´¢éƒ½ä¼šè¿›è¡Œä¸€æ¬¡é»˜è®¤æ–‡ä»¶çš„å‘é‡åŒ–ã€‚
        """

        # è®¾ç½®åˆå§‹åŒ–å‘é‡æ•°æ®åº“æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ã€‚å½“ç‚¹å‡»æ—¶ï¼Œè°ƒç”¨ get_vectordb å‡½æ•°ï¼Œå¹¶ä¼ å…¥ç”¨æˆ·çš„æ–‡ä»¶å’Œå¸Œæœ›ä½¿ç”¨çš„ Embedding æ¨¡å‹ã€‚
        init_db.click(get_vectordb_info, inputs=[file, embedding_model], outputs=[msg])

        # è®¾ç½®æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ã€‚å½“ç‚¹å‡»æ—¶ï¼Œè°ƒç”¨ä¸Šé¢å®šä¹‰çš„ chat_qa_chain_self_answer å‡½æ•°ï¼Œå¹¶ä¼ å…¥ç”¨æˆ·çš„æ¶ˆæ¯å’ŒèŠå¤©å†å²è®°å½•ï¼Œç„¶åæ›´æ–°æ–‡æœ¬æ¡†å’ŒèŠå¤©æœºå™¨äººç»„ä»¶ã€‚
        db_with_his_btn.click(model_center.chat_qa_chain_self_answer,
                              inputs=[msg, chatbot, model, embedding_model, temperature, top_k], outputs=[msg, chatbot])

        # è®¾ç½®æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ã€‚å½“ç‚¹å‡»æ—¶ï¼Œè°ƒç”¨ä¸Šé¢å®šä¹‰çš„ qa_chain_self_answer å‡½æ•°ï¼Œå¹¶ä¼ å…¥ç”¨æˆ·çš„æ¶ˆæ¯å’ŒèŠå¤©å†å²è®°å½•ï¼Œç„¶åæ›´æ–°æ–‡æœ¬æ¡†å’ŒèŠå¤©æœºå™¨äººç»„ä»¶ã€‚
        db_wo_his_btn.click(model_center.qa_chain_self_answer,
                            inputs=[msg, chatbot, model, embedding_model, temperature, top_k], outputs=[msg, chatbot])

        # è®¾ç½®æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ã€‚å½“ç‚¹å‡»æ—¶ï¼Œè°ƒç”¨ä¸Šé¢å®šä¹‰çš„ respond å‡½æ•°ï¼Œå¹¶ä¼ å…¥ç”¨æˆ·çš„æ¶ˆæ¯å’ŒèŠå¤©å†å²è®°å½•ï¼Œç„¶åæ›´æ–°æ–‡æœ¬æ¡†å’ŒèŠå¤©æœºå™¨äººç»„ä»¶ã€‚
        llm_btn.click(respond, inputs=[msg, chatbot, model, history_len, temperature], outputs=[msg, chatbot])

        # è®¾ç½®æ–‡æœ¬æ¡†çš„æäº¤äº‹ä»¶ï¼ˆå³æŒ‰ä¸‹Enteré”®æ—¶ï¼‰ã€‚åŠŸèƒ½ä¸ä¸Šé¢çš„ llm_btn æŒ‰é’®ç‚¹å‡»äº‹ä»¶ç›¸åŒã€‚
        msg.submit(respond, inputs=[msg, chatbot, model, history_len, temperature], outputs=[msg, chatbot])

        # ç‚¹å‡»åæ¸…ç©ºåç«¯å­˜å‚¨çš„èŠå¤©è®°å½•
        clear.click(model_center.clear_history)

    gr.Markdown("""æé†’ï¼š<br>
    1. ä½¿ç”¨æ—¶è¯·å…ˆä¸Šä¼ è‡ªå·±çš„çŸ¥è¯†æ–‡ä»¶ï¼Œä¸ç„¶å°†ä¼šè§£æé¡¹ç›®è‡ªå¸¦çš„çŸ¥è¯†åº“ã€‚
    2. åˆå§‹åŒ–æ•°æ®åº“æ—¶é—´å¯èƒ½è¾ƒé•¿ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚
    3. ä½¿ç”¨ä¸­å¦‚æœå‡ºç°å¼‚å¸¸ï¼Œå°†ä¼šåœ¨æ–‡æœ¬è¾“å…¥æ¡†è¿›è¡Œå±•ç¤ºï¼Œè¯·ä¸è¦æƒŠæ…Œã€‚ <br>
    """)
# threads to consume the request
gr.close_all()
# å¯åŠ¨æ–°çš„ Gradio åº”ç”¨ï¼Œè®¾ç½®åˆ†äº«åŠŸèƒ½ä¸º Trueï¼Œå¹¶ä½¿ç”¨ç¯å¢ƒå˜é‡ PORT1 æŒ‡å®šæœåŠ¡å™¨ç«¯å£ã€‚
# demo.launch(share=True, server_port=int(os.environ['PORT1']))
# ç›´æ¥å¯åŠ¨
demo.launch()
